{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09e44d-62ad-40d5-952c-8927d12d5867",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-13T15:45:27.664504Z"
    }
   },
   "outputs": [],
   "source": [
    "! python two_tower_recall.py\n",
    "#  topk:  10  :  hit_num:  10 hit_rate:  0.0002 user_num :  50000\n",
    " # topk:  20  :  hit_num:  21 hit_rate:  0.00042 user_num :  50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8ee19-2483-403f-b41d-418eefd1b932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "! python two_tower_item_embedding_recall.py --max_epochs 1 --negsample 0\n",
    "# topk:  10  :  hit_num:  31 hit_rate:  0.00062 user_num :  50000\n",
    "#  topk:  20  :  hit_num:  55 hit_rate:  0.0011 user_num :  50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cff8ef72b21c5cf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:24:15.479470Z",
     "start_time": "2024-04-14T13:20:52.086480Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/wlf/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "2024-04-15 11:26:58.032843: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-15 11:26:59.922119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[2024-04-15 11:27:02,394] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Max Epochs: 1\n",
      "Negative Samples: 0\n",
      "user_profile_     user_id\n",
      "0         1\n",
      "1        10\n",
      "2        21\n",
      "3        22\n",
      "4        24\n",
      "5        33\n",
      "6        41\n",
      "9        42\n",
      "11       51\n",
      "12       61\n",
      "item_profile_     click_article_id\n",
      "0             289197\n",
      "1              50644\n",
      "2             205958\n",
      "3             107014\n",
      "4             211442\n",
      "5             331116\n",
      "6              70986\n",
      "7              70758\n",
      "9              87209\n",
      "10             87560\n",
      "item_profile_         click_article_id\n",
      "0                 289197\n",
      "1                  50644\n",
      "2                 205958\n",
      "3                 107014\n",
      "4                 211442\n",
      "...                  ...\n",
      "229857            144978\n",
      "229858            285371\n",
      "229862            237224\n",
      "229885              2885\n",
      "229890            237289\n",
      "\n",
      "[13452 rows x 1 columns]\n",
      "item_profile         click_article_id\n",
      "0                  11002\n",
      "1                   1909\n",
      "2                   7895\n",
      "3                   4241\n",
      "4                   8172\n",
      "...                  ...\n",
      "229857              5561\n",
      "229858             10751\n",
      "229862              9128\n",
      "229885                66\n",
      "229890              9136\n",
      "\n",
      "[13452 rows x 1 columns]\n",
      "item_index_2_rawid [(11002, 289197), (1909, 50644), (7895, 205958), (4241, 107014), (8172, 211442), (12391, 331116), (2861, 70986), (2842, 70758), (3554, 87209), (3578, 87560)]\n"
     ]
    }
   ],
   "source": [
    "# criterion=bce\n",
    "! python two_tower_item_embedding_recall.py --max_epochs 1 --negsample 0\n",
    "# topk:  10  :  hit_num:  48 hit_rate:  0.00096 user_num :  50000\n",
    "# topk:  20  :  hit_num:  87 hit_rate:  0.00174 user_num :  50000\n",
    "# train_set[0] (14326, [5837, 9046], 13115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd325b04cf9c0153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T08:33:53.742438Z",
     "start_time": "2024-04-14T08:22:32.342908Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/wlf/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "2024-04-14 16:22:38.790990: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 16:22:40.714512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "[2024-04-14 16:22:43,207] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "Max Epochs: 1\r\n",
      "Negative Samples: 1\r\n",
      "100%|████████████████████████████████████| 50000/50000 [06:15<00:00, 133.15it/s]\r\n",
      "GPU available: True, used: False\r\n",
      "TPU available: False, using: 0 TPU cores\r\n",
      "IPU available: False, using: 0 IPUs\r\n",
      "HPU available: False, using: 0 HPUs\r\n",
      "\r\n",
      "  | Name           | Type       | Params\r\n",
      "----------------------------------------------\r\n",
      "0 | embedding_dict | ModuleDict | 2.0 M \r\n",
      "1 | user_dnn       | DNN        | 6.2 K \r\n",
      "2 | item_dnn       | DNN        | 20.2 K\r\n",
      "----------------------------------------------\r\n",
      "2.1 M     Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "2.1 M     Total params\r\n",
      "8.228     Total estimated model params size (MB)\r\n",
      "Epoch 0: 100%|█| 1254/1254 [03:10<00:00,  6.58it/s, loss=0.321, v_num=53, train_\r\n",
      "all_item_emb.shape (13452, 250)\r\n",
      "torch.Size([50000, 32])\r\n",
      "torch.Size([13452, 32])\r\n",
      "50000it [00:07, 7042.88it/s]\r\n",
      " topk:  10  :  hit_num:  37 hit_rate:  0.00074 user_num :  50000\r\n",
      " topk:  20  :  hit_num:  49 hit_rate:  0.00098 user_num :  50000\r\n"
     ]
    }
   ],
   "source": [
    "# criterion=bce\n",
    "! python two_tower_item_embedding_recall.py --max_epochs 1 --negsample 1\n",
    "# topk:  10  :  hit_num:  37 hit_rate:  0.00074 user_num :  50000\n",
    "#  topk:  20  :  hit_num:  49 hit_rate:  0.00098 user_num :  50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57bc72126f8524f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T03:10:46.780880Z",
     "start_time": "2024-04-14T02:58:10.021703Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-14 10:58:16.640541: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 10:58:18.841154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "Max Epochs: 1\r\n",
      "Negative Samples: 2\r\n",
      "100%|████████████████████████████████████| 50000/50000 [06:12<00:00, 134.25it/s]\r\n",
      "GPU available: True (cuda), used: False\r\n",
      "TPU available: False, using: 0 TPU cores\r\n",
      "IPU available: False, using: 0 IPUs\r\n",
      "HPU available: False, using: 0 HPUs\r\n",
      "\r\n",
      "  | Name           | Type       | Params\r\n",
      "----------------------------------------------\r\n",
      "0 | embedding_dict | ModuleDict | 2.0 M \r\n",
      "1 | user_dnn       | DNN        | 6.2 K \r\n",
      "2 | item_dnn       | DNN        | 20.2 K\r\n",
      "----------------------------------------------\r\n",
      "2.1 M     Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "2.1 M     Total params\r\n",
      "8.228     Total estimated model params size (MB)\r\n",
      "Epoch 0: 100%|█| 1841/1841 [04:45<00:00,  6.45it/s, loss=0.231, v_num=41, train_`Trainer.fit` stopped: `max_epochs=1` reached.\r\n",
      "Epoch 0: 100%|█| 1841/1841 [04:45<00:00,  6.44it/s, loss=0.231, v_num=41, train_\r\n",
      "all_item_emb.shape (13452, 250)\r\n",
      "torch.Size([50000, 32])\r\n",
      "torch.Size([13452, 32])\r\n",
      "50000it [00:04, 10130.04it/s]\r\n",
      " topk:  10  :  hit_num:  28 hit_rate:  0.00056 user_num :  50000\r\n",
      " topk:  20  :  hit_num:  34 hit_rate:  0.00068 user_num :  50000\r\n"
     ]
    }
   ],
   "source": [
    "! python two_tower_item_embedding_recall.py --max_epochs 1 --negsample 2\n",
    "# topk:  10  :  hit_num:  28 hit_rate:  0.00056 user_num :  50000\n",
    "# topk:  20  :  hit_num:  34 hit_rate:  0.00068 user_num :  50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3622318a3919a130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T05:39:13.168688Z",
     "start_time": "2024-04-14T05:17:46.279724Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-14 13:17:53.527930: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 13:17:55.416638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "Max Epochs: 1\r\n",
      "Negative Samples: 4\r\n",
      "100%|████████████████████████████████████| 50000/50000 [06:41<00:00, 124.66it/s]\r\n",
      "GPU available: True (cuda), used: False\r\n",
      "TPU available: False, using: 0 TPU cores\r\n",
      "IPU available: False, using: 0 IPUs\r\n",
      "HPU available: False, using: 0 HPUs\r\n",
      "\r\n",
      "  | Name           | Type       | Params\r\n",
      "----------------------------------------------\r\n",
      "0 | embedding_dict | ModuleDict | 2.0 M \r\n",
      "1 | user_dnn       | DNN        | 6.2 K \r\n",
      "2 | item_dnn       | DNN        | 20.2 K\r\n",
      "----------------------------------------------\r\n",
      "2.1 M     Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "2.1 M     Total params\r\n",
      "8.228     Total estimated model params size (MB)\r\n",
      "Epoch 0: 100%|█| 3016/3016 [12:52<00:00,  3.90it/s, loss=0.179, v_num=44, train_`Trainer.fit` stopped: `max_epochs=1` reached.\r\n",
      "Epoch 0: 100%|█| 3016/3016 [12:52<00:00,  3.90it/s, loss=0.179, v_num=44, train_\r\n",
      "all_item_emb.shape (13452, 250)\r\n",
      "torch.Size([50000, 32])\r\n",
      "torch.Size([13452, 32])\r\n",
      "50000it [00:06, 7927.80it/s] \r\n",
      " topk:  10  :  hit_num:  46 hit_rate:  0.00092 user_num :  50000\r\n",
      " topk:  20  :  hit_num:  202 hit_rate:  0.00404 user_num :  50000\r\n"
     ]
    }
   ],
   "source": [
    "! python two_tower_item_embedding_recall.py --max_epochs 1 --negsample 4\n",
    "#  topk:  10  :  hit_num:  46 hit_rate:  0.00092 user_num :  50000\n",
    "#  topk:  20  :  hit_num:  202 hit_rate:  0.00404 user_num :  50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c7a9ace89769b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T06:20:30.895312Z",
     "start_time": "2024-04-14T05:48:29.988926Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-14 13:48:36.302094: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 13:48:38.557428: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "Max Epochs: 1\r\n",
      "Negative Samples: 8\r\n",
      "100%|████████████████████████████████████| 50000/50000 [06:39<00:00, 125.05it/s]\r\n",
      "GPU available: True (cuda), used: False\r\n",
      "TPU available: False, using: 0 TPU cores\r\n",
      "IPU available: False, using: 0 IPUs\r\n",
      "HPU available: False, using: 0 HPUs\r\n",
      "\r\n",
      "  | Name           | Type       | Params\r\n",
      "----------------------------------------------\r\n",
      "0 | embedding_dict | ModuleDict | 2.0 M \r\n",
      "1 | user_dnn       | DNN        | 6.2 K \r\n",
      "2 | item_dnn       | DNN        | 20.2 K\r\n",
      "----------------------------------------------\r\n",
      "2.1 M     Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "2.1 M     Total params\r\n",
      "8.228     Total estimated model params size (MB)\r\n",
      "Epoch 0: 100%|█| 5364/5364 [23:07<00:00,  3.87it/s, loss=0.111, v_num=45, train_`Trainer.fit` stopped: `max_epochs=1` reached.\r\n",
      "Epoch 0: 100%|█| 5364/5364 [23:07<00:00,  3.87it/s, loss=0.111, v_num=45, train_\r\n",
      "all_item_emb.shape (13452, 250)\r\n",
      "torch.Size([50000, 32])\r\n",
      "torch.Size([13452, 32])\r\n",
      "50000it [00:06, 8208.26it/s] \r\n",
      " topk:  10  :  hit_num:  73 hit_rate:  0.00146 user_num :  50000\r\n",
      " topk:  20  :  hit_num:  75 hit_rate:  0.0015 user_num :  50000\r\n"
     ]
    }
   ],
   "source": [
    "! python two_tower_item_embedding_recall.py --max_epochs 1 --negsample 8\n",
    "#  topk:  10  :  hit_num:  73 hit_rate:  0.00146 user_num :  50000\n",
    "#  topk:  20  :  hit_num:  75 hit_rate:  0.0015 user_num :  50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e37848b231fa9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--negsample 4 BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb917c1ae069efbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:49:54.975869Z",
     "start_time": "2024-04-14T13:44:20.856444Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/wlf/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "2024-04-14 21:44:26.069155: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 21:44:27.954289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "[2024-04-14 21:44:30,430] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "Max Epochs: 2\r\n",
      "Negative Samples: 4\r\n",
      "GPU available: True, used: True\r\n",
      "TPU available: False, using: 0 TPU cores\r\n",
      "IPU available: False, using: 0 IPUs\r\n",
      "HPU available: False, using: 0 HPUs\r\n",
      "2024-04-14 21:46:17.832355: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 21:46:17.859320: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 21:46:18.696771: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 21:46:19.059596: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 21:46:19.641651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "2024-04-14 21:46:20.350818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "2024-04-14 21:46:21.136081: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "2024-04-14 21:46:21.352539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "[2024-04-14 21:46:21,984] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2024-04-14 21:46:23,450] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2024-04-14 21:46:23,609] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2024-04-14 21:46:23,857] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\r\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\r\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\r\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "distributed_backend=nccl\r\n",
      "All distributed processes registered. Starting with 4 processes\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "\r\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n",
      "Adam (\r\n",
      "Parameter Group 0\r\n",
      "    amsgrad: False\r\n",
      "    betas: (0.9, 0.999)\r\n",
      "    capturable: False\r\n",
      "    differentiable: False\r\n",
      "    eps: 1e-08\r\n",
      "    foreach: None\r\n",
      "    fused: False\r\n",
      "    lr: 0.001\r\n",
      "    maximize: False\r\n",
      "    weight_decay: 0\r\n",
      ")\r\n",
      "\r\n",
      "  | Name           | Type       | Params\r\n",
      "----------------------------------------------\r\n",
      "0 | embedding_dict | ModuleDict | 2.0 M \r\n",
      "1 | user_dnn       | DNN        | 6.2 K \r\n",
      "2 | item_dnn       | DNN        | 20.2 K\r\n",
      "----------------------------------------------\r\n",
      "2.1 M     Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "2.1 M     Total params\r\n",
      "8.228     Total estimated model params size (MB)\r\n",
      "Adam (\r\n",
      "Parameter Group 0\r\n",
      "    amsgrad: False\r\n",
      "    betas: (0.9, 0.999)\r\n",
      "    capturable: False\r\n",
      "    differentiable: False\r\n",
      "    eps: 1e-08\r\n",
      "    foreach: None\r\n",
      "    fused: False\r\n",
      "    lr: 0.001\r\n",
      "    maximize: False\r\n",
      "    weight_decay: 0\r\n",
      ")Adam (\r\n",
      "Parameter Group 0\r\n",
      "    amsgrad: False\r\n",
      "    betas: (0.9, 0.999)\r\n",
      "    capturable: False\r\n",
      "    differentiable: False\r\n",
      "    eps: 1e-08\r\n",
      "    foreach: None\r\n",
      "    fused: False\r\n",
      "    lr: 0.001\r\n",
      "    maximize: False\r\n",
      "    weight_decay: 0\r\n",
      ")\r\n",
      "\r\n",
      "Adam (\r\n",
      "Parameter Group 0\r\n",
      "    amsgrad: False\r\n",
      "    betas: (0.9, 0.999)\r\n",
      "    capturable: False\r\n",
      "    differentiable: False\r\n",
      "    eps: 1e-08\r\n",
      "    foreach: None\r\n",
      "    fused: False\r\n",
      "    lr: 0.001\r\n",
      "    maximize: False\r\n",
      "    weight_decay: 0\r\n",
      ")\r\n",
      "Epoch 0:   0%|                                          | 0/754 [00:00<?, ?it/s][W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "Epoch 0:   0%|                                  | 1/754 [00:03<42:40,  3.40s/it][W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "Epoch 1: 100%|█| 754/754 [01:01<00:00, 12.30it/s, loss=0.236, v_num=56, train_lo\r\n",
      "all_item_emb.shape (13452, 250)\r\n",
      "torch.Size([50000, 32])\r\n",
      "torch.Size([13452, 32])\r\n",
      "50000it [00:06, 8241.48it/s]\r\n",
      " topk:  10  :  hit_num:  39 hit_rate:  0.00078 user_num :  50000\r\n",
      " topk:  20  :  hit_num:  61 hit_rate:  0.00122 user_num :  50000\r\n"
     ]
    }
   ],
   "source": [
    "# ! python two_tower_item_embedding_recall.py --max_epochs 4 --negsample 4\n",
    "# topk:  10  :  hit_num:  14 hit_rate:  0.00028 user_num :  50000\n",
    "#  topk:  20  :  hit_num:  38 hit_rate:  0.00076 user_num :  50000\n",
    "# ! python two_tower_item_embedding_recall.py --max_epochs 2 --negsample 4\n",
    "# topk:  10  :  hit_num:  39 hit_rate:  0.00078 user_num :  50000\n",
    "#  topk:  20  :  hit_num:  61 hit_rate:  0.00122 user_num :  50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c29851f5736f71",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a60789ca1177a58",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b7502dbc49a048e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T14:15:19.727726Z",
     "start_time": "2024-04-14T14:08:12.446351Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/wlf/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "2024-04-14 22:08:18.083937: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 22:08:20.017129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "[2024-04-14 22:08:22,504] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "Max Epochs: 1\r\n",
      "Negative Samples: 4\r\n",
      "GPU available: True, used: True\r\n",
      "TPU available: False, using: 0 TPU cores\r\n",
      "IPU available: False, using: 0 IPUs\r\n",
      "HPU available: False, using: 0 HPUs\r\n",
      "2024-04-14 22:10:09.625190: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 22:10:09.735818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 22:10:09.741710: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 22:10:09.772985: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 22:10:11.420247: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "2024-04-14 22:10:11.569647: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "2024-04-14 22:10:11.638766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "2024-04-14 22:10:11.685128: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "[2024-04-14 22:10:13,777] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2024-04-14 22:10:14,000] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2024-04-14 22:10:14,053] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2024-04-14 22:10:14,118] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\r\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\r\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\r\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "distributed_backend=nccl\r\n",
      "All distributed processes registered. Starting with 4 processes\r\n",
      "----------------------------------------------------------------------------------------------------\r\n",
      "\r\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\r\n",
      "Adam (\r\n",
      "Parameter Group 0\r\n",
      "    amsgrad: False\r\n",
      "    betas: (0.9, 0.999)\r\n",
      "    capturable: False\r\n",
      "    differentiable: False\r\n",
      "    eps: 1e-08\r\n",
      "    foreach: None\r\n",
      "    fused: False\r\n",
      "    lr: 0.001\r\n",
      "    maximize: False\r\n",
      "    weight_decay: 0\r\n",
      ")Adam (\r\n",
      "Parameter Group 0\r\n",
      "    amsgrad: False\r\n",
      "    betas: (0.9, 0.999)\r\n",
      "    capturable: False\r\n",
      "    differentiable: False\r\n",
      "    eps: 1e-08\r\n",
      "    foreach: None\r\n",
      "    fused: False\r\n",
      "    lr: 0.001\r\n",
      "    maximize: False\r\n",
      "    weight_decay: 0\r\n",
      ")Adam (\r\n",
      "Parameter Group 0\r\n",
      "    amsgrad: False\r\n",
      "    betas: (0.9, 0.999)\r\n",
      "    capturable: False\r\n",
      "    differentiable: False\r\n",
      "    eps: 1e-08\r\n",
      "    foreach: None\r\n",
      "    fused: False\r\n",
      "    lr: 0.001\r\n",
      "    maximize: False\r\n",
      "    weight_decay: 0\r\n",
      ")\r\n",
      "\r\n",
      "\r\n",
      "Adam (\r\n",
      "Parameter Group 0\r\n",
      "    amsgrad: False\r\n",
      "    betas: (0.9, 0.999)\r\n",
      "    capturable: False\r\n",
      "    differentiable: False\r\n",
      "    eps: 1e-08\r\n",
      "    foreach: None\r\n",
      "    fused: False\r\n",
      "    lr: 0.001\r\n",
      "    maximize: False\r\n",
      "    weight_decay: 0\r\n",
      ")\r\n",
      "\r\n",
      "  | Name           | Type       | Params\r\n",
      "----------------------------------------------\r\n",
      "0 | embedding_dict | ModuleDict | 2.0 M \r\n",
      "1 | user_dnn       | DNN        | 6.2 K \r\n",
      "2 | item_dnn       | DNN        | 20.2 K\r\n",
      "----------------------------------------------\r\n",
      "2.1 M     Trainable params\r\n",
      "0         Non-trainable params\r\n",
      "2.1 M     Total params\r\n",
      "8.228     Total estimated model params size (MB)\r\n",
      "Epoch 0:   0%|                                         | 0/3016 [00:00<?, ?it/s][W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "Epoch 0:   0%|                               | 1/3016 [00:04<3:41:08,  4.40s/it][W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "[W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\r\n",
      "Epoch 0: 100%|█| 3016/3016 [03:38<00:00, 13.81it/s, loss=0.245, v_num=59, train_\r\n",
      "all_item_emb.shape (13452, 250)\r\n",
      "torch.Size([50000, 32])\r\n",
      "torch.Size([13452, 32])\r\n",
      "50000it [00:07, 6998.20it/s]\r\n",
      " topk:  10  :  hit_num:  20 hit_rate:  0.0004 user_num :  50000\r\n",
      " topk:  20  :  hit_num:  46 hit_rate:  0.00092 user_num :  50000\r\n"
     ]
    }
   ],
   "source": [
    "# 加大模型user_dnn_hidden_units=(256,64, embedding_dim)\n",
    "# ! python two_tower_item_embedding_recall.py --max_epochs 1 --negsample 4\n",
    "# topk:  10  :  hit_num:  18 hit_rate:  0.00036 user_num :  50000\n",
    "#  topk:  20  :  hit_num:  58 hit_rate:  0.00116 user_num :  50000\n",
    "\n",
    "# lr=1\n",
    "# ! python two_tower_item_embedding_recall.py --max_epochs 1 --negsample 4\n",
    "# topk:  10  :  hit_num:  20 hit_rate:  0.0004 user_num :  50000\n",
    "#  topk:  20  :  hit_num:  52 hit_rate:  0.00104 user_num :  50000\n",
    "\n",
    "# batch_size=64\n",
    "# ! python two_tower_item_embedding_recall.py --max_epochs 1 --negsample 4\n",
    "# topk:  10  :  hit_num:  20 hit_rate:  0.0004 user_num :  50000\n",
    "#  topk:  20  :  hit_num:  46 hit_rate:  0.00092 user_num :  50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d81c3e403e83a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "不用PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67ff085194613eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:10:04.329146Z",
     "start_time": "2024-04-14T13:08:18.076247Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/wlf/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "2024-04-14 21:08:23.402788: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-14 21:08:25.303797: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "Max Epochs: 1\r\n",
      "Negative Samples: 0\r\n",
      "100%|███████████████████████████████████| 50000/50000 [00:06<00:00, 7280.21it/s]\r\n",
      " 15%|██████                                    | 97/667 [00:03<00:09, 60.09it/s]Epoch [1, 100] loss: 0.5889431196451187\r\n",
      " 29%|███████████▉                             | 195/667 [00:04<00:07, 59.83it/s]Epoch [1, 200] loss: 0.5063649001717567\r\n",
      " 44%|██████████████████                       | 294/667 [00:06<00:06, 57.32it/s]Epoch [1, 300] loss: 0.4405502897500992\r\n",
      " 59%|████████████████████████▎                | 396/667 [00:08<00:05, 47.39it/s]Epoch [1, 400] loss: 0.388581175506115\r\n",
      " 75%|██████████████████████████████▌          | 497/667 [00:10<00:04, 41.91it/s]Epoch [1, 500] loss: 0.34969720512628555\r\n",
      " 90%|████████████████████████████████████▊    | 599/667 [00:13<00:01, 46.90it/s]Epoch [1, 600] loss: 0.3156735596060753\r\n",
      "100%|█████████████████████████████████████████| 667/667 [00:14<00:00, 46.46it/s]\r\n",
      "Finished Training\r\n",
      "all_item_emb.shape (13452, 250)\r\n",
      "torch.Size([50000, 32])\r\n",
      "torch.Size([13452, 32])\r\n",
      "50000it [00:04, 10014.61it/s]\r\n",
      " topk:  10  :  hit_num:  10 hit_rate:  0.0002 user_num :  50000\r\n",
      " topk:  20  :  hit_num:  26 hit_rate:  0.00052 user_num :  50000\r\n"
     ]
    }
   ],
   "source": [
    "! python two_tower_recall_torch.py --max_epochs 1 --negsample 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e9ba7c81a2dbe05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T14:16:57.077219Z",
     "start_time": "2024-04-16T14:16:21.915232Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/wlf/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "2024-04-16 22:16:24.914680: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n",
      "2024-04-16 22:16:26.089931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n",
      "Please check the latest version manually on https://pypi.org/project/deepctr-torch/#history\r\n",
      "[2024-04-16 22:16:27,841] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "Max Epochs: 1\r\n",
      "Negative Samples: 0\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/DataDisk1/wenlingfeng/Jupyter/RecSys/NewsRec/two_tower_custom.py\", line 540, in <module>\r\n",
      "    user_multi_recall_dict['youtubednn_recall'] = youtubednn_u2i_dict(hist_click_df, item_emb_df, topk=20,\r\n",
      "  File \"/DataDisk1/wenlingfeng/Jupyter/RecSys/NewsRec/two_tower_custom.py\", line 345, in youtubednn_u2i_dict\r\n",
      "    print('user_profile_', user_profile_[:10])\r\n",
      "NameError: name 'user_profile_' is not defined\r\n"
     ]
    }
   ],
   "source": [
    "! python two_tower_custom.py --max_epochs 1 --negsample 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe9c4285df58c7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
